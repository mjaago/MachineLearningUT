{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Homework 2 (deadline: October 8th, 10:15)\n",
    "## Regression, regularization and cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this homework you will experiment with linear regression and see what happens when we use regularized versions of it. More precisely, you will try out Ridge and Lasso regularization. In addition, we will see how using cross-validation helps us to get more stable estimates for our performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the data in **data.csv** and split it into training (50%) and testing (50%) set. Use random seed 0 (train_test_split method)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import shit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data.csv\", index_col = 0)\n",
    "## YOUR CODE\n",
    "## Add intercept\n",
    "X = data.drop('y', axis=1)\n",
    "y = data[['y']]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, test_size=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1. Multivariate linear regression (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='purple'>(a) Implement the fitting procedure of non-regularized multivariate ordinary least squares linear regression, as presented in the lecture slides (matrix operations). Fit on the training data and save the coefficients and the intercept for use in subtask (1c). Print out the coefficients corresponding to the five first features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients of five first features according to my implementation: [[11.85875833]\n",
      " [ 7.7562925 ]\n",
      " [ 9.63087962]\n",
      " [ 6.99932448]\n",
      " [ 4.99498325]]\n"
     ]
    }
   ],
   "source": [
    "## YOUR CODE\n",
    "X_train_w0 = X_train.assign(intercept = 1)\n",
    "w = np.linalg.inv(X_train_w0.T.dot(X_train_w0)).dot(X_train_w0.T.dot(y_train))\n",
    "intercept, coeff = w[-1], w[:-1]\n",
    "print('Coefficients of five first features according to my implementation:', w[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='purple'>(b) Call out the `sklearn.linear_model.LinearRegression` learning algorithm from the sklearn package. Fit the model on the training data and save it for use in the following subtasks. Print out the coefficients corresponding to the five first features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients of five first features according to sklearn: [11.85875833  7.7562925   9.63087962  6.99932448  4.99498325]\n"
     ]
    }
   ],
   "source": [
    "## YOUR CODE\n",
    "lr = LinearRegression().fit(X_train, y_train)\n",
    "print('Coefficients of five first features according to sklearn:', lr.coef_[0][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='purple'>(c) Demonstrate that the methods of subtasks (1a) and (1b) give the same results by showing that they find the same coefficients (don't forget the intercept). </font>\n",
    "\n",
    "You maybe won't get exactly the same results because of precision problems of floats so feel free to compare if the values are equal up to some precision (e.g. check if difference is less than 0.0000001 etc). You can use code similar to what has been given below, but you might need to change some things depending on where the intercept is in your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(abs(intercept - lr.intercept_) < 0.000001)\n",
    "for i in range(99):\n",
    "    assert(abs(coeff[i] - lr.coef_[0, i]) < 0.000001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='purple'>(d) Using the sklearn model from subtask (1b) predict the results on the training and testing set and calculate and show the root mean square errors (RMSE). Since you need to do the same evaluation in future tasks also, please implement a function 'evaluate' for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#################\n",
      "\n",
      "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n",
      "         normalize=False) \n",
      "\n",
      "RMSE train: y    2.407999e-14\n",
      "dtype: float64\n",
      "RMSE test: y    7.715131\n",
      "dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(y    2.407999e-14\n",
       " dtype: float64, y    7.715131\n",
       " dtype: float64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def evaluate(regression_model, trainX, trainY, testX, testY, verbose=True):\n",
    "    ## YOUR CODE\n",
    "    mse_tr = ((regression_model.predict(trainX) - trainY)**2).mean()\n",
    "    mse_te = ((regression_model.predict(testX) - testY)**2).mean()\n",
    "    \n",
    "    rmse_tr = np.sqrt(mse_tr)\n",
    "    rmse_te = np.sqrt(mse_te)\n",
    "    \n",
    "    if(verbose):\n",
    "        print(\"\\n#################\\n\")\n",
    "        print(regression_model, '\\n')\n",
    "        print(\"RMSE train:\", rmse_tr)\n",
    "        print(\"RMSE test:\", rmse_te)\n",
    "    \n",
    "    return rmse_tr, rmse_te\n",
    "\n",
    "evaluate(lr, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2. Ridge & Lambda regularized regression  (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This blogpost might clarify regularization a bit: https://towardsdatascience.com/l1-and-l2-regularization-methods-ce25e7fc831c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='purple'>(a) Implement the fitting procedure of ridge regression, as presented in the lecture slides (matrix operations). Fit on the training data with regularization parameter equal to 1 and save the coefficients and the intercept for use in subtask (2c). Print out the coefficients corresponding to the five first features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 100)\n",
      "Coefficients of five first features according to my implementation: [[7.91026896]\n",
      " [7.37023604]\n",
      " [6.97156822]\n",
      " [6.03240044]\n",
      " [4.42825097]]\n"
     ]
    }
   ],
   "source": [
    "## YOUR CODE\n",
    "print(X_train_w0.shape)\n",
    "reg_par = 1\n",
    "w = np.linalg.inv(X_train_w0.T.dot(X_train_w0) + reg_par * np.identity(X_train_w0.shape[0])).dot(X_train_w0.T.dot(y_train))\n",
    "rid_intercept, rid_coeff = w[-1], w[:-1]\n",
    "print('Coefficients of five first features according to my implementation:', rid_coeff[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='purple'>(b) Call out the `sklearn.linear_model.Ridge` learning algorithm from the sklearn package. Fit the model on the training data with regularization parameter equal to 1 and save it for use in the following subtasks. Print out the coefficients corresponding to the five first features.</font>\n",
    "\n",
    "Use parameters `solver = \"cholesky\", tol = 0.000000000001` in order to get more similar results to your own implementation. The default parameter for the regularization is already 1 so no need to specify that. The parameters `solver` and `tol` are necessary to force sklearn to use closed-form solution. Otherwise it would use numerical optimization which would give more different results from yours. **In the future tasks, please use the default option and don't force it to use the closed-form solution (numerical will be faster!).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients of five first features according to my implementation: [[7.91026896]\n",
      " [7.37023604]\n",
      " [6.97156822]\n",
      " [6.03240044]\n",
      " [4.42825097]]\n"
     ]
    }
   ],
   "source": [
    "## YOUR CODE\n",
    "rid = Ridge(solver=\"cholesky\", tol=0.000000000001).fit(X_train, y_train)\n",
    "print('Coefficients of five first features according to my implementation:', rid_coeff[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='purple'>(c) Demonstrate the correctess of your implementation the same way as in the previous exercise. For this compare your coefficients and intercept as obtained in subtask (2a) with the coeffiecients and intercept from sklearn, as obtained in subtask (2b). The results can actually vary quite a bit due to implementation differences in matrix operations. Compare that the differences in results (coefficients and intercept) are less than 0.02."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE\n",
    "assert(abs(rid_intercept - rid.intercept_) < 0.02)\n",
    "for i in range(99):\n",
    "    assert(abs(rid_coeff[i] - rid.coef_[0, i]) < 0.02)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='purple'>(d) Train a Lasso model using the sklearn package (use the default regularization parameter) and save it for future use. Print out the coefficients corresponding to the five first features.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients of five first features according to my implementation: [[7.91026896]\n",
      " [7.37023604]\n",
      " [6.97156822]\n",
      " [6.03240044]\n",
      " [4.42825097]]\n"
     ]
    }
   ],
   "source": [
    "## YOUR CODE\n",
    "lasso = Lasso().fit(X_train, y_train)\n",
    "print('Coefficients of five first features according to my implementation:', rid_coeff[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='purple'>(e) Evaluate the sklearn Ridge and Lasso models on the training and testing set and calculate and show the RMSE, using the function 'evaluate' from subtask (1d)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate Ridge: \n",
      "\n",
      "#################\n",
      "\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='cholesky', tol=1e-12) \n",
      "\n",
      "RMSE train: y    0.515938\n",
      "dtype: float64\n",
      "RMSE test: y    4.44622\n",
      "dtype: float64\n",
      "Evaluate Lasso: \n",
      "\n",
      "#################\n",
      "\n",
      "Lasso(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=None,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False) \n",
      "\n",
      "RMSE train: 4.984130516628728\n",
      "RMSE test: 5.400343154987264\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4.984130516628728, 5.400343154987264)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## YOUR CODE\n",
    "print(\"Evaluate Ridge: \")\n",
    "evaluate(rid, X_train, y_train, X_test, y_test)\n",
    "\n",
    "print(\"Evaluate Lasso: \")\n",
    "evaluate(lasso, X_train, y_train['y'], X_test, y_test['y'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3. Choosing a suitable regularization parameter  (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since different parameters can lead to very different results we need to do some parameter tuning and find a suitable regularization parameter for both Ridge and Lasso. We could try out different values and see which ones lead to the best results on the test set. However, then we would overfit to our test data and we would not have an adequate estimate of how good the model is in the end. That is why we need to do parameter tuning only using the training set.\n",
    "\n",
    "Use **alphas = np.linspace(0.01, 10, 100)** for Ridge and **alphas = np.linspace(0.01, 5, 100)** for Lasso. The method generates 100 values with equal steps between the first and second parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='purple'>(a) **Method 1:** Divide the training set into training and validation set using 90%/10% split and a random seed 0 (train_test_split method). Train Ridge and Lasso models with different alpha values on the training set and calculate the RMSE values on the validation set. Choose and report the alpha that has the best RMSE for Ridge and another alpha that has best RMSE for Lasso (save both alpha and RMSE values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRMSE(model, alpha, X_train, y_train, X_test, y_test):\n",
    "    w = model.set_params(alpha = alpha).fit(X_train, y_train)\n",
    "    _, test_rmse = evaluate(model, X_train, y_train, X_test, y_test, False)\n",
    "    return test_rmse\n",
    "\n",
    "def getXandYFromData(data):\n",
    "    X = data.drop('y', axis=1)\n",
    "    y = data['y']\n",
    "    return X, y\n",
    "\n",
    "def getTrainTestSplit(data, testSize, random_seed):\n",
    "    X,y = getXandYFromData(data)\n",
    "    return train_test_split(X, y, random_state=random_seed, test_size=testSize)\n",
    "\n",
    "def getBestAlphaRmseForAlphas(model, alphas, X_train, y_train, X_test, y_test):\n",
    "    rmse, best_alpha = -1, 0\n",
    "    for alpha in alphas:\n",
    "        rmse_tmp = getRMSE(model, alpha, X_train, y_train, X_test, y_test)\n",
    "        if rmse_tmp < rmse or rmse == -1:\n",
    "            rmse, best_alpha = rmse_tmp, alpha\n",
    "    return best_alpha,rmse\n",
    "\n",
    "def getKFoldRmseSum(kf, model, data, alpha):\n",
    "    rmse_sum = 0\n",
    "    X,y = getXandYFromData(data)\n",
    "    for train_indices, test_indices in kf.split(X, y):\n",
    "        X_train, X_test = X.iloc[train_indices], X.iloc[test_indices]\n",
    "        y_train, y_test = y.iloc[train_indices], y.iloc[test_indices]\n",
    "        rmse_sum += getRMSE(model, alpha, X_train, y_train, X_test, y_test)\n",
    "    return rmse_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.01, 0.7491922102868802)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def method_1(model, data, alphas, random_seed):\n",
    "    X_train, X_test, y_train, y_test = getTrainTestSplit(data, 0.1, random_seed)\n",
    "    return getBestAlphaRmseForAlphas(model, alphas, X_train, y_train, X_test, y_test)\n",
    "\n",
    "rid_alphas = np.linspace(0.01, 10, 100)\n",
    "lasso_alphas = np.linspace(0.01, 5, 100)\n",
    "\n",
    "\n",
    "method_1(Ridge(), data, rid_alphas, 0) ## ridge\n",
    "method_1(Lasso(), data, lasso_alphas, 0) ## lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='purple'>(b) **Method 2:** Instead of doing only one training/validation split, use 10-fold cross validation. For each alpha value calculate the validation errors for each of the folds and average the results. Then choose and report the alpha that has the best RMSE for Ridge and another alpha that has best RMSE for Lasso (save both alpha and RMSE values). For doing the 10-fold split use the sklearn method KFold (kf = KFold(n_splits=10, random_state = 0, shuffle = True)). To see more about how to iterate through the folds see the documentation for the method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.01, 0.6933687187795554)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def method_2(model, alphas, data, random_seed):\n",
    "    kf = KFold(n_splits=10, random_state = random_seed, shuffle = True)\n",
    "    rmse, best_alpha = -1, 0\n",
    "    for alpha in alphas:\n",
    "        rmse_sum = getKFoldRmseSum(kf, model, data, alpha)\n",
    "        avg_rmse = rmse_sum / kf.get_n_splits()\n",
    "        if avg_rmse < rmse or rmse == -1:\n",
    "            rmse, best_alpha = avg_rmse, alpha\n",
    "    return best_alpha,rmse\n",
    "\n",
    "method_2(Ridge(), rid_alphas, data, 0) ## ridge\n",
    "method_2(Lasso(), lasso_alphas, data, 0) ## lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4. Comparing the stability of Method 1 and Method 2  (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='purple'>(a) Run Method 1 and Method 2 both 10 times, every time using a different value 0,1,2,...,9 as the random_state. Report the best alpha and RMSE for both parameter tuning methods and for both regularization methods for each of the 10 trials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Regularization method</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>Alpha</th>\n",
       "      <th>Random state</th>\n",
       "      <th>Method idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Lasso</td>\n",
       "      <td>0.749192</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lasso</td>\n",
       "      <td>0.693369</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>0.872231</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>0.884071</td>\n",
       "      <td>0.110808</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lasso</td>\n",
       "      <td>0.552962</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Lasso</td>\n",
       "      <td>0.661993</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>0.849399</td>\n",
       "      <td>0.312424</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>0.806203</td>\n",
       "      <td>0.262020</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Lasso</td>\n",
       "      <td>0.650384</td>\n",
       "      <td>0.060404</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Lasso</td>\n",
       "      <td>0.679472</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>0.862762</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>0.848389</td>\n",
       "      <td>0.110808</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Lasso</td>\n",
       "      <td>0.688056</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Lasso</td>\n",
       "      <td>0.688048</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>0.947384</td>\n",
       "      <td>0.262020</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>0.870792</td>\n",
       "      <td>0.161212</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Lasso</td>\n",
       "      <td>0.682129</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Lasso</td>\n",
       "      <td>0.690135</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>0.740645</td>\n",
       "      <td>0.362828</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>0.884307</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Lasso</td>\n",
       "      <td>0.574299</td>\n",
       "      <td>0.060404</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Lasso</td>\n",
       "      <td>0.677100</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>0.809074</td>\n",
       "      <td>0.413232</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>0.886845</td>\n",
       "      <td>0.110808</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Lasso</td>\n",
       "      <td>0.692406</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Lasso</td>\n",
       "      <td>0.681710</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>0.797594</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>0.868226</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Lasso</td>\n",
       "      <td>0.711342</td>\n",
       "      <td>0.060404</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Lasso</td>\n",
       "      <td>0.687854</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>0.929684</td>\n",
       "      <td>0.161212</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>0.873889</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Lasso</td>\n",
       "      <td>0.592106</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Lasso</td>\n",
       "      <td>0.674997</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>0.647873</td>\n",
       "      <td>0.866869</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>0.844929</td>\n",
       "      <td>0.060404</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Lasso</td>\n",
       "      <td>0.614405</td>\n",
       "      <td>0.060404</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Lasso</td>\n",
       "      <td>0.690256</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>0.920863</td>\n",
       "      <td>0.161212</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>0.834195</td>\n",
       "      <td>0.110808</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Regularization method      RMSE     Alpha  Random state  Method idx\n",
       "0                  Lasso  0.749192  0.010000             0           1\n",
       "1                  Lasso  0.693369  0.010000             0           2\n",
       "2                  Ridge  0.872231  0.010000             0           1\n",
       "3                  Ridge  0.884071  0.110808             0           2\n",
       "4                  Lasso  0.552962  0.010000             1           1\n",
       "5                  Lasso  0.661993  0.010000             1           2\n",
       "6                  Ridge  0.849399  0.312424             1           1\n",
       "7                  Ridge  0.806203  0.262020             1           2\n",
       "8                  Lasso  0.650384  0.060404             2           1\n",
       "9                  Lasso  0.679472  0.010000             2           2\n",
       "10                 Ridge  0.862762  0.010000             2           1\n",
       "11                 Ridge  0.848389  0.110808             2           2\n",
       "12                 Lasso  0.688056  0.010000             3           1\n",
       "13                 Lasso  0.688048  0.010000             3           2\n",
       "14                 Ridge  0.947384  0.262020             3           1\n",
       "15                 Ridge  0.870792  0.161212             3           2\n",
       "16                 Lasso  0.682129  0.010000             4           1\n",
       "17                 Lasso  0.690135  0.010000             4           2\n",
       "18                 Ridge  0.740645  0.362828             4           1\n",
       "19                 Ridge  0.884307  0.010000             4           2\n",
       "20                 Lasso  0.574299  0.060404             5           1\n",
       "21                 Lasso  0.677100  0.010000             5           2\n",
       "22                 Ridge  0.809074  0.413232             5           1\n",
       "23                 Ridge  0.886845  0.110808             5           2\n",
       "24                 Lasso  0.692406  0.010000             6           1\n",
       "25                 Lasso  0.681710  0.010000             6           2\n",
       "26                 Ridge  0.797594  0.010000             6           1\n",
       "27                 Ridge  0.868226  0.010000             6           2\n",
       "28                 Lasso  0.711342  0.060404             7           1\n",
       "29                 Lasso  0.687854  0.010000             7           2\n",
       "30                 Ridge  0.929684  0.161212             7           1\n",
       "31                 Ridge  0.873889  0.010000             7           2\n",
       "32                 Lasso  0.592106  0.010000             8           1\n",
       "33                 Lasso  0.674997  0.010000             8           2\n",
       "34                 Ridge  0.647873  0.866869             8           1\n",
       "35                 Ridge  0.844929  0.060404             8           2\n",
       "36                 Lasso  0.614405  0.060404             9           1\n",
       "37                 Lasso  0.690256  0.010000             9           2\n",
       "38                 Ridge  0.920863  0.161212             9           1\n",
       "39                 Ridge  0.834195  0.110808             9           2"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasso_alphas = np.linspace(0.01, 5, 100)\n",
    "rid_alphas = np.linspace(0.01, 10, 100)\n",
    "results = {'Regularization method': [], 'RMSE': [], 'Alpha':[], 'Random state': [], 'Method idx': []}\n",
    "\n",
    "def append_result(results, reg_method, rmse, alpha, rand_state, method):\n",
    "    results['Regularization method'].append(reg_method) \n",
    "    results['RMSE'].append(rmse)\n",
    "    results['Alpha'].append(alpha)\n",
    "    results['Random state'].append(rand_state)\n",
    "    results['Method idx'].append(method)\n",
    "\n",
    "for i in range(10):\n",
    "    rmse_for_alpha = {}\n",
    "    lasso_alpha_1, lasso_rmse_1 = method_1(Lasso(), data, lasso_alphas, i)\n",
    "    lasso_alpha_2, lasso_rmse_2 = method_2(Lasso(), lasso_alphas, data, i) \n",
    "    append_result(results, 'Lasso', lasso_rmse_1, lasso_alpha_1, i, 1)\n",
    "    append_result(results, 'Lasso', lasso_rmse_2, lasso_alpha_2, i, 2)\n",
    "    \n",
    "    rid_alpha_1, rid_rmse_1 = method_1(Ridge(), data, lasso_alphas, i) \n",
    "    rid_alpha_2, rid_rmse_2 = method_2(Ridge(), lasso_alphas, data, i) \n",
    "    append_result(results, 'Ridge', rid_rmse_1, rid_alpha_1, i, 1)\n",
    "    append_result(results, 'Ridge', rid_rmse_2, rid_alpha_2, i, 2)\n",
    "    \n",
    "result_df = pd.DataFrame(results)\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Regularization method</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>Alpha</th>\n",
       "      <th>Random state</th>\n",
       "      <th>Method idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lasso</td>\n",
       "      <td>0.552962</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Lasso</td>\n",
       "      <td>0.574299</td>\n",
       "      <td>0.060404</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Lasso</td>\n",
       "      <td>0.592106</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Lasso</td>\n",
       "      <td>0.614405</td>\n",
       "      <td>0.060404</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>0.647873</td>\n",
       "      <td>0.866869</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Lasso</td>\n",
       "      <td>0.650384</td>\n",
       "      <td>0.060404</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Lasso</td>\n",
       "      <td>0.661993</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Lasso</td>\n",
       "      <td>0.674997</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Lasso</td>\n",
       "      <td>0.677100</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Lasso</td>\n",
       "      <td>0.679472</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Lasso</td>\n",
       "      <td>0.681710</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Lasso</td>\n",
       "      <td>0.682129</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Lasso</td>\n",
       "      <td>0.687854</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Lasso</td>\n",
       "      <td>0.688048</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Lasso</td>\n",
       "      <td>0.688056</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Lasso</td>\n",
       "      <td>0.690135</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Lasso</td>\n",
       "      <td>0.690256</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Lasso</td>\n",
       "      <td>0.692406</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lasso</td>\n",
       "      <td>0.693369</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Lasso</td>\n",
       "      <td>0.711342</td>\n",
       "      <td>0.060404</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>0.740645</td>\n",
       "      <td>0.362828</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Lasso</td>\n",
       "      <td>0.749192</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>0.797594</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>0.806203</td>\n",
       "      <td>0.262020</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>0.809074</td>\n",
       "      <td>0.413232</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>0.834195</td>\n",
       "      <td>0.110808</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>0.844929</td>\n",
       "      <td>0.060404</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>0.848389</td>\n",
       "      <td>0.110808</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>0.849399</td>\n",
       "      <td>0.312424</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>0.862762</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>0.868226</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>0.870792</td>\n",
       "      <td>0.161212</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>0.872231</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>0.873889</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>0.884071</td>\n",
       "      <td>0.110808</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>0.884307</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>0.886845</td>\n",
       "      <td>0.110808</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>0.920863</td>\n",
       "      <td>0.161212</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>0.929684</td>\n",
       "      <td>0.161212</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>0.947384</td>\n",
       "      <td>0.262020</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Regularization method      RMSE     Alpha  Random state  Method idx\n",
       "4                  Lasso  0.552962  0.010000             1           1\n",
       "20                 Lasso  0.574299  0.060404             5           1\n",
       "32                 Lasso  0.592106  0.010000             8           1\n",
       "36                 Lasso  0.614405  0.060404             9           1\n",
       "34                 Ridge  0.647873  0.866869             8           1\n",
       "8                  Lasso  0.650384  0.060404             2           1\n",
       "5                  Lasso  0.661993  0.010000             1           2\n",
       "33                 Lasso  0.674997  0.010000             8           2\n",
       "21                 Lasso  0.677100  0.010000             5           2\n",
       "9                  Lasso  0.679472  0.010000             2           2\n",
       "25                 Lasso  0.681710  0.010000             6           2\n",
       "16                 Lasso  0.682129  0.010000             4           1\n",
       "29                 Lasso  0.687854  0.010000             7           2\n",
       "13                 Lasso  0.688048  0.010000             3           2\n",
       "12                 Lasso  0.688056  0.010000             3           1\n",
       "17                 Lasso  0.690135  0.010000             4           2\n",
       "37                 Lasso  0.690256  0.010000             9           2\n",
       "24                 Lasso  0.692406  0.010000             6           1\n",
       "1                  Lasso  0.693369  0.010000             0           2\n",
       "28                 Lasso  0.711342  0.060404             7           1\n",
       "18                 Ridge  0.740645  0.362828             4           1\n",
       "0                  Lasso  0.749192  0.010000             0           1\n",
       "26                 Ridge  0.797594  0.010000             6           1\n",
       "7                  Ridge  0.806203  0.262020             1           2\n",
       "22                 Ridge  0.809074  0.413232             5           1\n",
       "39                 Ridge  0.834195  0.110808             9           2\n",
       "35                 Ridge  0.844929  0.060404             8           2\n",
       "11                 Ridge  0.848389  0.110808             2           2\n",
       "6                  Ridge  0.849399  0.312424             1           1\n",
       "10                 Ridge  0.862762  0.010000             2           1\n",
       "27                 Ridge  0.868226  0.010000             6           2\n",
       "15                 Ridge  0.870792  0.161212             3           2\n",
       "2                  Ridge  0.872231  0.010000             0           1\n",
       "31                 Ridge  0.873889  0.010000             7           2\n",
       "3                  Ridge  0.884071  0.110808             0           2\n",
       "19                 Ridge  0.884307  0.010000             4           2\n",
       "23                 Ridge  0.886845  0.110808             5           2\n",
       "38                 Ridge  0.920863  0.161212             9           1\n",
       "30                 Ridge  0.929684  0.161212             7           1\n",
       "14                 Ridge  0.947384  0.262020             3           1"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df.sort_values(by=['RMSE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='purple'>(b) What can you say about the stability of the methods? Which one gives more stable information about which alpha to use? Which alpha values turn out to be best in the end for these data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='purple'>(c) Create two plots (one for Ridge and one for Lasso) where on each plot there are two boxplots - one for showing the distribution of the RMSE values for the 10 trials for Method 1 and the other for Method 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'whiskers': [<matplotlib.lines.Line2D at 0x7f33e2fbbf28>,\n",
       "  <matplotlib.lines.Line2D at 0x7f33e2fbbef0>,\n",
       "  <matplotlib.lines.Line2D at 0x7f33e2ec0518>,\n",
       "  <matplotlib.lines.Line2D at 0x7f33e2ec0860>],\n",
       " 'caps': [<matplotlib.lines.Line2D at 0x7f33e3921a20>,\n",
       "  <matplotlib.lines.Line2D at 0x7f33e2eec7b8>,\n",
       "  <matplotlib.lines.Line2D at 0x7f33e2ec0ba8>,\n",
       "  <matplotlib.lines.Line2D at 0x7f33e2ec0ef0>],\n",
       " 'boxes': [<matplotlib.lines.Line2D at 0x7f33e2fbbb38>,\n",
       "  <matplotlib.lines.Line2D at 0x7f33e2eecef0>],\n",
       " 'medians': [<matplotlib.lines.Line2D at 0x7f33e2eecb00>,\n",
       "  <matplotlib.lines.Line2D at 0x7f33e2ec0eb8>],\n",
       " 'fliers': [<matplotlib.lines.Line2D at 0x7f33e2eece48>,\n",
       "  <matplotlib.lines.Line2D at 0x7f33e32585c0>],\n",
       " 'means': []}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAADTdJREFUeJzt3U+InPd9x/H3p3ZMoHHMbqUWI6leHRQTYQqCwT0YkjStQfHBTlsoUiFQMPHJJjFtwaElUQSll5Kc1IMgprikFiYJQQeDCo1LSnGCZh0nRBIyQsX1RgGvY7lOTq7CtwetzHS9q3lmd0aP9jfvFwzszPw0z1fS8taj386fVBWSpLb8Rt8DSJKmz7hLUoOMuyQ1yLhLUoOMuyQ1yLhLUoOMuyQ1yLhLUoOMuyQ16M6+Drxr165aWlrq6/CStCMtLy+/VVW7x63rLe5LS0sMh8O+Di9JO1KS17usc1tGkhpk3CWpQcZdkhrU2567JpdkS7/Ot3WW5o9x30FuFukkRlzS+9yWkaQGGXdJapBxl6QGGXdJapBxl6QGGXdJapBxl6QGGXdJapBxl6QGGXdJapBxl6QGGXdJapBxl6QGGXdJapBxl6QGGXdJapBxl6QGdYp7ksNJLia5lOSZDe7/epJX1y6vJXln+qNKkroa+zF7Se4ATgAPAyvA2SSnq+r8jTVV9fTI+qeAQzOYVZLUUZcz9weBS1V1uareA04Bj91k/VHg+WkMJ0nami5x3wO8MXJ9Ze22D0hyH7Af+N4m9z+RZJhkuLq6OumskqSOusQ9G9xWm6w9Anyrqn690Z1VdbKqBlU12L17d9cZJUkT6hL3FWDfyPW9wJVN1h7BLRlJ6t3YH6gCZ4EDSfYDP+N6wP98/aIk9wMLwMtTnVDSjpBs9J/8m6vabBNA2zX2zL2qrgFPAmeAC8ALVXUuyfEkj44sPQqcKv+2tm1xcZEkE12AidYvLi72/LtUa6pqw8u4+zQb6esPeDAY1HA47OXYt7skM//GvxXHUJsWFxe5evXqTI+xsLDA22+/PdNj7FRJlqtqMG5dl20ZSXrf1atXb8nJh7bHuEuaSH3lo3DsntkfQ9ti3CVNJF9999ZsGx6b6SGa5xuHSVKDjLskNchtGUkTm/UPPBcWFmb6+PPAuEuayKT77T7tth9uy0hSg4y7JDXIuEtSg9xzvw35IhFJ22Xcb0O+SETSdhl3SVNxs6dHbnafz6KZHeMuaSoM9e3FH6hKUoOMuyQ1yLhLUoOMuyQ1yLhLUoOMuyQ1yLhLUoOMuyQ1yLhLUoOMuyQ1yLhLUoOMuyQ1yLhLUoOMuyQ1yLhLUoOMuyQ1yLhLUoOMuyQ1yLhLUoOMuyQ1yLhLUoOMuyQ1yLhLUoM6xT3J4SQXk1xK8swma/4syfkk55L8y3THlCRN4s5xC5LcAZwAHgZWgLNJTlfV+ZE1B4AvAQ9V1dUkvz2rgSVJ43U5c38QuFRVl6vqPeAU8Ni6NZ8HTlTVVYCqenO6Y0qSJtEl7nuAN0aur6zdNupjwMeS/GeSHyQ5vNEDJXkiyTDJcHV1dWsTS5LGGrstA2SD22qDxzkAfArYC/xHkgeq6p3/94uqTgInAQaDwfrH0Ihkoz/26VlYWJjp40vqV5e4rwD7Rq7vBa5ssOYHVfW/wH8lucj12J+dypRzpmryf/eSbOnXSWpTl22Zs8CBJPuT3AUcAU6vW/Nd4A8Akuzi+jbN5WkOKknqbmzcq+oa8CRwBrgAvFBV55IcT/Lo2rIzwC+SnAdeAv66qn4xq6ElSTeXvv4rPxgMajgc9nLsFrktI82HJMtVNRi3zleoSlKDjLskNci4S1KDjLskNci4S1KDjLskNci4S1KDjLskNci4S1KDjLskNci4S1KDjLskNci4S1KDjLskNci4S1KDjLskNajLZ6jqNjHuQ7M3u98P8ZDmj3HfQYy0pK7clpGkBhl3SWpQbx+QnWQVeL2Xg7dpF/BW30NIG/B7c7ruq6rd4xb1FndNV5Jhl09El241vzf74baMJDXIuEtSg4x7O072PYC0Cb83e+CeuyQ1yDN3SWqQcZekBhn3HS7Js0neTPLTvmeRRiXZl+SlJBeSnEvyhb5nmifuue9wST4B/Ap4rqoe6Hse6YYk9wL3VtUrSe4GloHPVtX5nkebC56573BV9X3g7b7nkNarqp9X1StrX/8SuADs6Xeq+WHcJc1ckiXgEPDDfieZH8Zd0kwl+QjwbeCLVfVu3/PMC+MuaWaSfIjrYf9mVX2n73nmiXGXNBO5/tFg3wAuVNXX+p5n3hj3HS7J88DLwP1JVpI83vdM0pqHgM8Bn07y6trlkb6Hmhc+FVKSGuSZuyQ1yLhLUoOMuyQ16M6+Drxr165aWlrq6/CStCMtLy+/1eUzVHuL+9LSEsPhsK/DS9KOlOT1LuvclpGkBhl3SWpQb9symtz1F/xNztcySPPHuO8gN4t0EiMu6X1uy0hSg4y7JDXIuEtSg4y7JDXIuEtSg4y7JDXIuEtSg4y7JDXIuEtSgzrFPcnhJBeTXEryzAb335fk35L8JMm/J9k7/VElSV2NjXuSO4ATwGeAg8DRJAfXLfsH4Lmq+j3gOPD30x50niwuLpJkogsw0frFxcWef5eSZqnLe8s8CFyqqssASU4BjwHnR9YcBJ5e+/ol4LvTHHLeXL16debvE7PVNyGTtDN02ZbZA7wxcn1l7bZRPwb+dO3rPwbuTvJb2x9PkrQVXeK+0Sne+tPKvwI+meRHwCeBnwHXPvBAyRNJhkmGq6urEw8rSeqmS9xXgH0j1/cCV0YXVNWVqvqTqjoE/M3abf+z/oGq6mRVDapqsHv32I8AlCRtUZe4nwUOJNmf5C7gCHB6dEGSXUluPNaXgGenO6YkaRJj415V14AngTPABeCFqjqX5HiSR9eWfQq4mOQ14HeAv5vRvJKkDtLXp/cMBoMaDoe9HPt2dys+VclPbtK0beUZWH4PTi7JclUNxq3zFaqSJrLZ6zC2wtdhzI6foSppIr4OY2fwzF2SGmTcJalBbstImkh95aNw7J7ZH0PbYtwlTSRffffWPJvr2EwP0Ty3ZSSpQcZdkhrktsxtyD1NSdtl3G9D7mlK2i63ZSSpQZ65S5rYrF9BurCwMNPHnwfGXdJEJt0y9E3q+uG2jCQ1yLhLUoOMuyQ1yLhLUoOMuyQ1yLhLUoOMuyQ1yLhLUoM6xT3J4SQXk1xK8swG9/9ukpeS/CjJT5I8Mv1RJUldjY17kjuAE8BngIPA0SQH1y37W+CFqjoEHAH+cdqDSpK663Lm/iBwqaouV9V7wCngsXVrCrjxHrL3AFemN6KknSDJhpdx92k2ury3zB7gjZHrK8Dvr1tzDPjXJE8Bvwn80VSmk7Rj+P4xt5cuZ+4b/fO6/m/xKPBPVbUXeAT45yQfeOwkTyQZJhmurq5OPu0c2exMZ1oX33VPaluXM/cVYN/I9b18cNvlceAwQFW9nOTDwC7gzdFFVXUSOAkwGAz8Z34TWzkD8p33JI3qcuZ+FjiQZH+Su7j+A9PT69b8N/CHAEk+DnwY8NRcknoyNu5VdQ14EjgDXOD6s2LOJTme5NG1ZX8JfD7Jj4Hngb8oTyMlqTedPqyjql4EXlx325dHvj4PPDTd0SRJW+UrVCWpQcZdkhpk3CWpQcZdkhpk3CWpQcZdkhpk3CWpQcZdkhpk3CWpQcZdkhpk3CWpQcZdkhpk3CWpQcZdkhpk3CWpQcZdkhpk3CWpQcZdkhpk3CWpQcZdkhpk3CWpQZ3inuRwkotJLiV5ZoP7v57k1bXLa0nemf6okqSu7hy3IMkdwAngYWAFOJvkdFWdv7Gmqp4eWf8UcGgGs0qSOupy5v4gcKmqLlfVe8Ap4LGbrD8KPD+N4SRJW9Ml7nuAN0aur6zd9gFJ7gP2A9/b/miSpK3qEvdscFttsvYI8K2q+vWGD5Q8kWSYZLi6utp1RknShLrEfQXYN3J9L3Blk7VHuMmWTFWdrKpBVQ12797dfUpJ0kS6xP0scCDJ/iR3cT3gp9cvSnI/sAC8PN0RJUmTGhv3qroGPAmcAS4AL1TVuSTHkzw6svQocKqqNtuykSTdImOfCglQVS8CL6677cvrrh+b3liSpO3wFaqS1CDjLkkNMu6S1CDjLkkNMu6S1CDjLkkNMu6S1CDjLkkNMu6S1CDjLkkN6vT2A7o9JBu9+/L4+327H2n+GPcdxEhL6sptGUlqkHGXpAYZd0lqUPrax02yCrzey8HbtAt4q+8hpA34vTld91XV2M8p7S3umq4kw6oa9D2HtJ7fm/1wW0aSGmTcJalBxr0dJ/seQNqE35s9cM9dkhrkmbskNci473BJnk3yZpKf9j2LNCrJviQvJbmQ5FySL/Q90zxxW2aHS/IJ4FfAc1X1QN/zSDckuRe4t6peSXI3sAx8tqrO9zzaXPDMfYerqu8Db/c9h7ReVf28ql5Z+/qXwAVgT79TzQ/jLmnmkiwBh4Af9jvJ/DDukmYqyUeAbwNfrKp3+55nXhh3STOT5ENcD/s3q+o7fc8zT4y7pJnI9Y8G+wZwoaq+1vc888a473BJngdeBu5PspLk8b5nktY8BHwO+HSSV9cuj/Q91LzwqZCS1CDP3CWpQcZdkhpk3CWpQcZdkhpk3CWpQcZdkhpk3CWpQcZdkhr0f03gSq4Swm/eAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## YOUR CODE\n",
    "fig, axs = plt.subplots(2)\n",
    "\n",
    "# Lasso boxplot\n",
    "lasso_1_rmse = result_df[(result_df['Regularization method'] == 'Lasso') & (result_df['Method idx'] == 1)]['RMSE']\n",
    "lasso_2_rmse = result_df[(result_df['Regularization method'] == 'Lasso') & (result_df['Method idx'] == 2)]['RMSE']\n",
    "axs[0].boxplot((lasso_1_rmse, lasso_2_rmse))\n",
    "\n",
    "# Ridge boxplot\n",
    "rid_1_rmse = result_df[(result_df['Regularization method'] == 'Ridge') & (result_df['Method idx'] == 1)]['RMSE']\n",
    "rid_2_rmse = result_df[(result_df['Regularization method'] == 'Ridge') & (result_df['Method idx'] == 2)]['RMSE']\n",
    "axs[1].boxplot((rid_1_rmse, rid_2_rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'whiskers': [<matplotlib.lines.Line2D at 0x7f33e2b2f978>,\n",
       "  <matplotlib.lines.Line2D at 0x7f33e2b2fcc0>,\n",
       "  <matplotlib.lines.Line2D at 0x7f33e2a88d30>,\n",
       "  <matplotlib.lines.Line2D at 0x7f33e2a95438>],\n",
       " 'caps': [<matplotlib.lines.Line2D at 0x7f33e2b2ff98>,\n",
       "  <matplotlib.lines.Line2D at 0x7f33e2a88390>,\n",
       "  <matplotlib.lines.Line2D at 0x7f33e2a95780>,\n",
       "  <matplotlib.lines.Line2D at 0x7f33e2a95ac8>],\n",
       " 'boxes': [<matplotlib.lines.Line2D at 0x7f33e2b2f588>,\n",
       "  <matplotlib.lines.Line2D at 0x7f33e2a88d68>],\n",
       " 'medians': [<matplotlib.lines.Line2D at 0x7f33e2a886d8>,\n",
       "  <matplotlib.lines.Line2D at 0x7f33e2a95e10>],\n",
       " 'fliers': [<matplotlib.lines.Line2D at 0x7f33e2a88a20>,\n",
       "  <matplotlib.lines.Line2D at 0x7f33e2a95dd8>],\n",
       " 'means': []}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## YOUR CODE\n",
    "\n",
    "# Lasso boxplot\n",
    "plt.figure()\n",
    "lasso_1_rmse = result_df[(result_df['Regularization method'] == 'Lasso') & (result_df['Method idx'] == 1)]['RMSE']\n",
    "lasso_2_rmse = result_df[(result_df['Regularization method'] == 'Lasso') & (result_df['Method idx'] == 2)]['RMSE']\n",
    "plt.title('Lasso')\n",
    "plt.boxplot((lasso_1_rmse, lasso_2_rmse))\n",
    "\n",
    "# Ridge boxplot\n",
    "plt.figure()\n",
    "rid_1_rmse = result_df[(result_df['Regularization method'] == 'Ridge') & (result_df['Method idx'] == 1)]['RMSE']\n",
    "rid_2_rmse = result_df[(result_df['Regularization method'] == 'Ridge') & (result_df['Method idx'] == 2)]['RMSE']\n",
    "plt.title('Ridge')\n",
    "plt.boxplot((rid_1_rmse, rid_2_rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='purple'>(d) Comment on why the results look like they do? In general, when tuning parameters, is it better to use one training-validation split or K-fold cross-validation? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5. Regularization parameter effect on the coefficients  (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='purple'>(a) The regularization parameter influences the values of the coefficients. Create two plots (one for Ridge and one for Lasso) that have the regularization parameter on the x-axis and coefficient values on the y-axis. You don't have to take all 99 values, you can take for example the first 20. Show each coefficient as a line (on the same plot) and comment on what happens when the regularization parameter increases. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='purple'>(b) What does Ridge regression do and what does Lasso regression do? How do they differ? </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6. Evaluating different models  (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='purple'>(a) Choose the values of alpha for Ridge and Lasso according to subtask (4b). Now let's see which model works best for our data by evaluating the test RMSE. Compare the following models by reporting the training and testing set RMSE: </font>\n",
    "\n",
    "1. Non-regularized linear regression\n",
    "2. Ridge regression with your chosen parameter\n",
    "3. Lasso regression with your chosen parameter\n",
    "4. A \"dumb\" model that always predicts the mean value of y_train\n",
    "5. An ideal model that the instructors have used for generating the data (the true coefficients are [10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0, 0, ..., 0] and intercept 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='purple'>(b) Which method gives the best results and by looking at which value do you claim that? Why did this method work the best in your opinion?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='purple'>(c) Were all of the \"smart\" models better than the \"dumb\" one. What would it mean if the learned model would give worse results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='purple'>(d) Were the learned models far from the ideal one? Were the learned coefficients similar to the true ones?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='purple'>(e) Which model overfitted the most, how can you see that?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='purple'>(f) Are regularized methods always better than methods without regularization (not only in this case but in general). Why/why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional Bonus Task 7. Dual perceptron (Bonus, 1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='purple'> Implement the dual perceptron algorithm. </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"dual_perceptron.png\" alt=\"Dual perceptron\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** check `pandas` [manual about indexing data](https://pandas.pydata.org/pandas-docs/stable/indexing.html) and [iterrows documentation](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.iterrows.html), it could be useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x1 = np.array([5, 6, 7, 7, 8, 9, 0, 1, 2, 4, 5, 6])\n",
    "x2 = np.array([2, 1, 3, 5, 10, 3, 4, 8, 6, 10, 9, 11])\n",
    "y = np.array([1, 1, 1, 1, 1, 1, -1, -1, -1, -1, -1, -1])\n",
    "\n",
    "data = pd.DataFrame({'x1': x1, 'x2': x2, 'y':y})\n",
    "data['x0'] = np.ones_like(data['x1']) # Don't forget to create homogeneous coordinates!\n",
    "data = data[['x0', 'x1', 'x2', 'y']]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dual_perceptron(data):\n",
    "    # Your code goes here\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w = dual_perceptron(data)\n",
    "xs= np.linspace(0,8,100)\n",
    "ys = (-1) * (w[1] / w[2]) * xs - w[0] / w[2]\n",
    "\n",
    "fig = plt.figure(figsize=(5,5))\n",
    "plt.plot(x1[y<0], x2[y<0], 'bo')\n",
    "plt.plot(x1[y>0], x2[y>0], 'ro')\n",
    "plt.yticks(np.arange(13))\n",
    "plt.xticks(np.arange(13))\n",
    "plt.plot(xs, ys, color = \"green\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
